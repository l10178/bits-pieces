<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Java进程内存分析 | bits-pieces</title><link rel=icon href=/bits-pieces/favicon/favicon-32x32.png type=image/x-icon><link rel=stylesheet href=/bits-pieces/main.min.css media=screen><link rel=stylesheet href=/bits-pieces/custom.css media=screen><link rel=alternate type=application/rss+xml href=https://l10178.github.io/bits-pieces/java/jvm/index.xml title=bits-pieces></head><body><div class=wrapper><input type=checkbox class=hidden id=menu-control><header class=gdoc-header><div class="container flex align-center justify-between"><label for=menu-control class=gdoc-nav__control><svg class="icon menu"><use xlink:href="#menu"/></svg><svg class="icon arrow-back"><use xlink:href="#arrow_back"/></svg></label><a class=gdoc-header__link href=https://l10178.github.io/bits-pieces/><span class="gdoc-brand flex align-center"><img class=gdoc-brand__img src=/bits-pieces/images/logos/logo.png alt=bits-pieces width=38 height=38>
bits-pieces</span></a></div></header><main class="container flex flex-even"><aside class=gdoc-nav><nav><div class=gdoc-search><svg class="icon search"><use xlink:href="#search"/></svg><input type=text id=gdoc-search-input class=gdoc-search__input placeholder=Search... aria-label=Search maxlength=64><div class="gdoc-search__spinner spinner hidden"></div><ul id=gdoc-search-results class=gdoc-search__list></ul></div><section class=gdoc-nav--main><h2>Navigation</h2><ul class=gdoc-nav__list><li><span class=flex><a href=/bits-pieces/devops/ class=gdoc-nav__entry>DevOps</a></span><ul class=gdoc-nav__list><li><span class=flex><a href=/bits-pieces/devops/mysql5.7to8.0/ class=gdoc-nav__entry>MySQL5.7升级至8.0</a></span></li><li><span class=flex><a href=/bits-pieces/devops/mysql-large-import/ class=gdoc-nav__entry>MySQL大文件导入优化</a></span></li><li><span class=flex><a href=/bits-pieces/devops/nexus/ class=gdoc-nav__entry>Nexus3 批量导入</a></span></li><li><span class=flex><a href=/bits-pieces/devops/superset/ class=gdoc-nav__entry>Superset 对接 ClickHouse</a></span></li><li><span class=flex><a href=/bits-pieces/devops/cdr/ class=gdoc-nav__entry>使用Visual Studio Code搭建多用户远程IDE</a></span></li></ul></li><li><span class=flex><a href=/bits-pieces/java/ class=gdoc-nav__entry>Java</a></span><ul class=gdoc-nav__list><li><span class=flex><a href=/bits-pieces/java/jvm/ class="gdoc-nav__entry is-active">Java进程内存分析</a></span></li><li><span class=flex><a href=/bits-pieces/java/spring-boot-micrometer/ class=gdoc-nav__entry>Spring Boot 使用 Micrometer 集成 Prometheus 监控</a></span></li><li><span class=flex><a href=/bits-pieces/java/spring-start-site/ class=gdoc-nav__entry>Spring Start脚手架快速入门</a></span></li><li><span class=flex><a href=/bits-pieces/java/spring-boot/ class=gdoc-nav__entry>从 Spring 到 Spring Boot</a></span></li></ul></li><li><span class=flex>Kubernetes</span><ul class=gdoc-nav__list><li><span class=flex><a href=/bits-pieces/kubernetes/envoy/ class=gdoc-nav__entry>Envoy生产配置最佳实践</a></span></li><li><span class=flex><a href=/bits-pieces/kubernetes/tldr/ class=gdoc-nav__entry>TL;DR</a></span></li><li><span class=flex><a href=/bits-pieces/kubernetes/cka/ class=gdoc-nav__entry>备考 CKA 过程，CKA 真题</a></span></li><li><span class=flex><a href=/bits-pieces/kubernetes/pid/ class=gdoc-nav__entry>容器进程数限制</a></span></li></ul></li><li><span class=flex><a href=/bits-pieces/linux/ class=gdoc-nav__entry>Linux</a></span><ul class=gdoc-nav__list><li><span class=flex><a href=/bits-pieces/linux/regular-expressions/ class=gdoc-nav__entry>Regular Expressions</a></span></li><li><span class=flex><a href=/bits-pieces/linux/git/ class=gdoc-nav__entry>Git常用配置</a></span></li><li><span class=flex><a href=/bits-pieces/linux/history/ class=gdoc-nav__entry>History入门</a></span></li><li><span class=flex><a href=/bits-pieces/linux/pngquant/ class=gdoc-nav__entry>PNG图片批量压缩</a></span></li><li><span class=flex><a href=/bits-pieces/linux/shell-coding/ class=gdoc-nav__entry>Shell 编程实用句式</a></span></li><li><span class=flex><a href=/bits-pieces/linux/tcpdump/ class=gdoc-nav__entry>tcpdump</a></span></li><li><span class=flex><a href=/bits-pieces/linux/transaction/ class=gdoc-nav__entry>数据库事务控制</a></span></li><li><span class=flex><a href=/bits-pieces/linux/tar.gz/ class=gdoc-nav__entry>文件压缩解压</a></span></li></ul></li><li><span class=flex><a href=/bits-pieces/windows/ class=gdoc-nav__entry>Windows</a></span><ul class=gdoc-nav__list><li><span class=flex><a href=/bits-pieces/windows/takeown/ class=gdoc-nav__entry>Windows提权 + 设置环境变量</a></span></li></ul></li><li><span class=flex><a href=/bits-pieces/knives/ class=gdoc-nav__entry>瑞士军刀</a></span></li></ul></section><section class=gdoc-nav--more><h2>More</h2><ul class=gdoc-nav__list><li><span class=flex><svg class="icon download"><use xlink:href="#download"/></svg><a href=https://github.com/l10178/bits-pieces/releases class=gdoc-nav__entry>Releases</a></span></li><li><span class=flex><svg class="icon github"><use xlink:href="#github"/></svg><a href=https://github.com/l10178/bits-pieces class=gdoc-nav__entry>View Source</a></span></li></ul></section></nav></aside><div class=gdoc-page><div class="gdoc-page__header flex flex-wrap justify-between" itemscope itemtype=https://schema.org/Breadcrumb><span></span><span><svg class="icon code"><use xlink:href="#code"/></svg><a href=https://github.com/l10178/bits-pieces/edit/main/content/java/jvm/_index.md>Edit this page</a></span></div><article class=gdoc-markdown><h1>Java进程内存分析</h1><p>故事背景：</p><p>一个 K8S Pod，里面只有一个 Java 进程，K8S request 和 limit memory 都是 2G，Java 进程核心参数包括：<code>-XX:+UseZGC -Xmx1024m -Xms768m -XX:SoftMaxHeapSize=512m</code>。</p><p>服务启动一段时间后，查看 Grafana 监控数据，Pod 内存使用量约 1.5G，JVM 内存使用量约 500M，通过 jvm dump 分析没有任何大对象，运行三五天后出现 ContainerOOM。</p><p>首先区分下 ContainerOOM 和 JvmOOM，ContainerOOM 是 Pod 内存不够，Java 向操作系统申请内存时内存不足导致。</p><p>问题来了：</p><ol><li>Pod 2G 内存，JVM 设置了 <code>Xmx 1G</code>，已经预留了 1G 内存，为什么还会 ContainerOOM，这预留的 1G 内存被谁吃了。</li><li>正常情况下（无 ContainerOOM），Grafana 看到的监控数据，Pod 内存使用量 1.5G， JVM 内存使用量 500M，差别为什么这么大。</li><li>Grafana 看到的监控数据，内存使用量、提交量各是什么意思，这些值是怎么算出来的，和 Pod 进程中如何对应，为什么提交量一直居高不小。</li></ol><p>Grafana 监控图。</p><p><a href=./grafana-pod-jvm.png><img src=./grafana-pod-jvm.png alt=Grafana监控图></a></p><div class=gdoc-page__anchorwrap><h2 id=统计指标>统计指标<a data-clipboard-text=https://l10178.github.io/bits-pieces/java/jvm/#统计指标 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor 统计指标" href=#统计指标><svg class="icon link"><use xlink:href="#link"/></svg></a></h2></div><p><code>Pod 内存使用量</code>统计的指标是 <code>container_memory_working_set_bytes</code>：</p><ul><li>container_memory_usage_bytes = container_memory_rss + container_memory_cache + kernel memory</li><li>container_memory_working_set_bytes = container_memory_usage_bytes - total_inactive_file（未激活的匿名缓存页）</li></ul><p>container_memory_working_set_bytes 是容器真实使用的内存量，也是资源限制 limit 时的 OOM 判断依据。</p><p>另外注意 cgroup 版本差异： <code>container_memory_cache</code> reflects <code>cache (cgroup v1)</code> or <code>file (cgroup v2)</code> entry in memory.stat.</p><p><code>JVM 内存使用量</code>统计的指标是 <code>jvm_memory_bytes_used</code>： heap、non-heap 以及<code>其他</code> 真实用量总和。下面解释其他。</p><p>对比一下 top 命令，使用 top 命令看一下 Java 进程真正占了多少。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>top -p $(pgrep java)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>#</span> 注意下面的数据和截图不是同一时间的
</span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
</span></span></span><span class=line><span class=cl><span class=go>     7 root      20   0   58.2g   1.6g   1.0g S  17.3  2.3 659:42.46 java
</span></span></span></code></pre></div><p>VIRT：virtual memory usage 虚拟内存</p><ol><li>进程“需要的”虚拟内存大小，包括进程使用的库、代码、数据等</li><li>假如进程申请 100m 的内存，但实际只使用了 10m，那么它会增长 100m，而不是实际的使用量</li></ol><p>RES：resident memory usage 常驻内存</p><ol><li>进程当前使用的内存大小，但不包括被换出到交换区的部分</li><li>包含其他进程的共享</li><li>如果申请 100m 的内存，实际使用 10m，它只增长 10m，与 VIRT 相反</li><li>关于库占用内存的情况，它只统计加载的库文件所占内存大小</li></ol><p>SHR：shared memory 共享内存</p><ol><li>除了自身进程的共享内存，也包括其他进程的共享内存</li><li>虽然进程只使用了几个共享库的函数，但它包含了整个共享库的大小</li><li>计算某个进程所占的物理内存大小公式：RES – SHR。（JVM 的内存使用量等于 RES-SHR）</li></ol><p>container_memory_working_set_bytes 与 Top RES 相等吗。</p><p>为什么 container_memory_working_set_bytes 大于 top RES:</p><p>因为 container_memory_working_set_bytes 包含 container_memory_cache，这涉及到 <code>Linux 缓存机制</code>，延伸阅读：<a href=https://zhuanlan.zhihu.com/p/449630026>https://zhuanlan.zhihu.com/p/449630026</a>。遇到这种场景一般都是文件操作较多，可优先排除文件类操作。</p><p>为什么 container_memory_working_set_bytes 小于 top RES:</p><p>主要还是算法和数据来源不一样，top 的 <code>RES=Code + Data</code>，有些服务 Data 比较大。 当然实际测试会发现 RES!=Code + Data ，延伸阅读：<a href=https://liam.page/2020/07/17/memory-stat-in-TOP/>https://liam.page/2020/07/17/memory-stat-in-TOP/</a></p><p>另外可能看到的现象，top、granfana、docker stats、JMX 看到的使用量怎么都不一样，都是因为他们统计的维度不一样。</p><p>所以通过 top 命令看到的数据不一定是真实的，container_memory_working_set_bytes 指标来自 cadvisor，cadvisor 数据来源 cgroup，可以查看以下文件获取真实的内存情况。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>#</span> 线上老版本，cgroup v1
</span></span><span class=line><span class=cl><span class=go>ll /sys/fs/cgroup/memory/
</span></span></span><span class=line><span class=cl><span class=go>total 0
</span></span></span><span class=line><span class=cl><span class=go>drwxr-xr-x.  2 root root   0 Dec 24 19:22 ./
</span></span></span><span class=line><span class=cl><span class=go>dr-xr-xr-x. 13 root root 340 Dec 24 19:22 ../
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 cgroup.clone_children
</span></span></span><span class=line><span class=cl><span class=go>--w--w--w-.  1 root root   0 Dec 24 19:22 cgroup.event_control
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 cgroup.procs
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.failcnt
</span></span></span><span class=line><span class=cl><span class=go>--w-------.  1 root root   0 Dec 24 19:22 memory.force_empty
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.failcnt
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.limit_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.max_usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.slabinfo
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.tcp.failcnt
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.tcp.limit_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.tcp.max_usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.tcp.usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r--.  1 root root   0 Dec 24 19:22 memory.kmem.usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.limit_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.max_usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.memsw.failcnt
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.memsw.limit_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.memsw.max_usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r--.  1 root root   0 Dec 24 19:22 memory.memsw.usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.move_charge_at_immigrate
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r--.  1 root root   0 Dec 24 19:22 memory.numa_stat
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.oom_control
</span></span></span><span class=line><span class=cl><span class=go>----------.  1 root root   0 Dec 24 19:22 memory.pressure_level
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.soft_limit_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r--.  1 root root   0 Dec 24 19:22 memory.stat
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.swappiness
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r--.  1 root root   0 Dec 24 19:22 memory.usage_in_bytes
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 memory.use_hierarchy
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 notify_on_release
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r--.  1 root root   0 Dec 24 19:22 tasks
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>#</span> 线下新版本，cgroup v2
</span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>ll /sys/fs/cgroup/memory.*
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.current
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.events
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.events.local
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  7 11:50 /sys/fs/cgroup/memory.high
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  7 11:50 /sys/fs/cgroup/memory.low
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  7 11:50 /sys/fs/cgroup/memory.max
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.min
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.numa_stat
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  7 11:50 /sys/fs/cgroup/memory.oom.group
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.pressure
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.stat
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.swap.current
</span></span></span><span class=line><span class=cl><span class=go>-r--r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.swap.events
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.swap.high
</span></span></span><span class=line><span class=cl><span class=go>-rw-r--r-- 1 root root 0 Jan  6 16:25 /sys/fs/cgroup/memory.swap.max
</span></span></span></code></pre></div><p>JVM 关于使用量和提交量的解释。</p><p><code>Used Size</code>：The used space is the amount of memory that is currently occupied by Java objects.
当前实际真的用着的内存，每个 bit 都对应了有值的。</p><p><code>Committed Size</code>：The committed size is the amount of memory guaranteed to be available for use by the Java virtual machine.<br>操作系统向 JVM 保证可用的内存大小，或者说 JVM 向操作系统已经要的内存。站在操作系统的角度，就是已经分出去（占用）的内存，保证给 JVM 用了，其他进程不能用了。 由于操作系统的内存管理是惰性的，对于已申请的内存虽然会分配地址空间，但并不会直接占用物理内存，真正使用的时候才会映射到实际的物理内存，所以 committed > res 也是很可能的。</p><div class=gdoc-page__anchorwrap><h2 id=java-进程内存分析>Java 进程内存分析<a data-clipboard-text=https://l10178.github.io/bits-pieces/java/jvm/#java-进程内存分析 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor Java 进程内存分析" href=#java-进程内存分析><svg class="icon link"><use xlink:href="#link"/></svg></a></h2></div><p>Pod 的内存使用量 1.5G，都包含哪些。</p><p>kernel memory 为 0，Cache 约 1100M，rss 约 650M，inactive_file 约 200M。可以看到 Cache 比较大，因为这个服务比较特殊有很多文件操作。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>#</span> 这个数据和上面的1.5G不是同时的。
</span></span><span class=line><span class=cl><span class=go>cat  /sys/fs/cgroup/memory/memory.stat
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>cache 1455861760
</span></span></span><span class=line><span class=cl><span class=go>rss 685862912
</span></span></span><span class=line><span class=cl><span class=go>rss_huge 337641472
</span></span></span><span class=line><span class=cl><span class=go>mapped_file 504979456
</span></span></span><span class=line><span class=cl><span class=go>swap 0
</span></span></span><span class=line><span class=cl><span class=go>inactive_anon 805306368
</span></span></span><span class=line><span class=cl><span class=go>active_anon 685817856
</span></span></span><span class=line><span class=cl><span class=go>inactive_file 299671552
</span></span></span><span class=line><span class=cl><span class=go>active_file 350883840
</span></span></span><span class=line><span class=cl><span class=go>total_rss 685862912
</span></span></span><span class=line><span class=cl><span class=go>total_rss_huge 337641472
</span></span></span><span class=line><span class=cl><span class=go>total_mapped_file 504979456
</span></span></span><span class=line><span class=cl><span class=go>total_inactive_file 299671552
</span></span></span><span class=line><span class=cl><span class=go>total_active_file 350883840
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>#</span> cgroup v2 变量变了
</span></span><span class=line><span class=cl><span class=go>cat /sys/fs/cgroup/memory.stat
</span></span></span><span class=line><span class=cl><span class=go>anon 846118912
</span></span></span><span class=line><span class=cl><span class=go>file 2321530880
</span></span></span><span class=line><span class=cl><span class=go>kernel_stack 10895360
</span></span></span><span class=line><span class=cl><span class=go>pagetables 15523840
</span></span></span><span class=line><span class=cl><span class=go>percpu 0
</span></span></span><span class=line><span class=cl><span class=go>sock 1212416
</span></span></span><span class=line><span class=cl><span class=go>shmem 1933574144
</span></span></span><span class=line><span class=cl><span class=go>file_mapped 1870290944
</span></span></span><span class=line><span class=cl><span class=go>file_dirty 12288
</span></span></span><span class=line><span class=cl><span class=go>file_writeback 0
</span></span></span><span class=line><span class=cl><span class=go>swapcached 0
</span></span></span><span class=line><span class=cl><span class=go>anon_thp 0
</span></span></span><span class=line><span class=cl><span class=go>file_thp 0
</span></span></span><span class=line><span class=cl><span class=go>shmem_thp 0
</span></span></span><span class=line><span class=cl><span class=go>inactive_anon 2602876928
</span></span></span><span class=line><span class=cl><span class=go>active_anon 176771072
</span></span></span><span class=line><span class=cl><span class=go>inactive_file 188608512
</span></span></span><span class=line><span class=cl><span class=go>active_file 199348224
</span></span></span><span class=line><span class=cl><span class=go>unevictable 0
</span></span></span><span class=line><span class=cl><span class=go>slab_reclaimable 11839688
</span></span></span><span class=line><span class=cl><span class=go>slab_unreclaimable 7409400
</span></span></span><span class=line><span class=cl><span class=go>slab 19249088
</span></span></span><span class=line><span class=cl><span class=go>workingset_refault_anon 0
</span></span></span><span class=line><span class=cl><span class=go>workingset_refault_file 318
</span></span></span><span class=line><span class=cl><span class=go>workingset_activate_anon 0
</span></span></span><span class=line><span class=cl><span class=go>workingset_activate_file 95
</span></span></span><span class=line><span class=cl><span class=go>workingset_restore_anon 0
</span></span></span><span class=line><span class=cl><span class=go>workingset_restore_file 0
</span></span></span><span class=line><span class=cl><span class=go>workingset_nodereclaim 0
</span></span></span><span class=line><span class=cl><span class=go>pgfault 2563565
</span></span></span><span class=line><span class=cl><span class=go>pgmajfault 15
</span></span></span><span class=line><span class=cl><span class=go>pgrefill 14672
</span></span></span><span class=line><span class=cl><span class=go>pgscan 25468
</span></span></span><span class=line><span class=cl><span class=go>pgsteal 25468
</span></span></span><span class=line><span class=cl><span class=go>pgactivate 106436
</span></span></span><span class=line><span class=cl><span class=go>pgdeactivate 14672
</span></span></span><span class=line><span class=cl><span class=go>pglazyfree 0
</span></span></span><span class=line><span class=cl><span class=go>pglazyfreed 0
</span></span></span><span class=line><span class=cl><span class=go>thp_fault_alloc 0
</span></span></span><span class=line><span class=cl><span class=go>thp_collapse_alloc 0
</span></span></span></code></pre></div><p>通过 Java 自带的 Native Memory Tracking 看下内存提交量。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Java启动时先打开NativeMemoryTracking，默认是关闭的。注意不要在生产环境长期开启，有性能损失</span>
</span></span><span class=line><span class=cl>java -XX:NativeMemoryTracking<span class=o>=</span>detail -jar
</span></span><span class=line><span class=cl><span class=c1># 查看详情</span>
</span></span><span class=line><span class=cl>jcmd <span class=k>$(</span>pgrep java<span class=k>)</span> VM.native_memory detail <span class=nv>scale</span><span class=o>=</span>MB
</span></span></code></pre></div><p>通过 Native Memory Tracking 追踪到的详情大致如下，关注其中每一项 <code>committed</code> 值。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>Native Memory Tracking:
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>(Omitting categories weighting less than 1MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>Total: reserved=68975MB, committed=1040MB
</span></span></span><span class=line><span class=cl><span class=go>-                 Java Heap (reserved=58944MB, committed=646MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=58944MB, committed=646MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                     Class (reserved=1027MB, committed=15MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (classes #19551)  #加载类的个数
</span></span></span><span class=line><span class=cl><span class=go>                            (  instance classes #18354, array classes #1197)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=3MB #63653)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=1024MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (  Metadata:   )
</span></span></span><span class=line><span class=cl><span class=go>                            (    reserved=96MB, committed=94MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (    used=93MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (    waste=0MB =0.40%)
</span></span></span><span class=line><span class=cl><span class=go>                            (  Class space:)
</span></span></span><span class=line><span class=cl><span class=go>                            (    reserved=1024MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (    used=11MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (    waste=1MB =4.63%)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                    Thread (reserved=337MB, committed=37MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (thread #335) #线程的个数
</span></span></span><span class=line><span class=cl><span class=go>                            (stack: reserved=336MB, committed=36MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #2018)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                      Code (reserved=248MB, committed=86MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=6MB #24750)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=242MB, committed=80MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                        GC (reserved=8243MB, committed=83MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=19MB #45814)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=8224MB, committed=64MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                  Compiler (reserved=3MB, committed=3MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=3MB #2212)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                  Internal (reserved=7MB, committed=7MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=7MB #31683)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                     Other (reserved=18MB, committed=18MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=18MB #663)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                    Symbol (reserved=19MB, committed=19MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=17MB #502325)
</span></span></span><span class=line><span class=cl><span class=go>                            (arena=2MB #1)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-    Native Memory Tracking (reserved=12MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #8073)
</span></span></span><span class=line><span class=cl><span class=go>                            (tracking overhead=11MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-        Shared class space (reserved=12MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=12MB, committed=12MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                    Module (reserved=1MB, committed=1MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #4996)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-           Synchronization (reserved=1MB, committed=1MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #2482)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-                 Metaspace (reserved=97MB, committed=94MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=1MB #662)
</span></span></span><span class=line><span class=cl><span class=go>                            (mmap: reserved=96MB, committed=94MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>-           Object Monitors (reserved=8MB, committed=8MB)
</span></span></span><span class=line><span class=cl><span class=go>                            (malloc=8MB #39137)
</span></span></span></code></pre></div><p>一图解千愁，盗图。</p><p><a href=./jvm-memory-structure.png><img src=./jvm-memory-structure.png alt=JVM内存结构></a></p><ul><li><p>Heap
Heap 是 Java 进程中使用量最大的一部分内存，是最常遇到内存问题的部分，Java 也提供了很多相关工具来排查堆内存泄露问题，这里不详细展开。Heap 与 RSS 相关的几个重要 JVM 参数如下：
Xms：Java Heap 初始内存大小。(目前我们用的百分比控制，MaxRAMPercentage)
Xmx：Java Heap 的最大大小。(InitialRAMPercentage)
XX:+UseAdaptiveSizePolicy：是否开启自适应大小策略。开启后，JVM 将动态判断是否调整 Heap size，来降低系统负载。</p></li><li><p>Metaspace
Metaspace 主要包含方法的字节码，Class 对象，常量池。一般来说，记载的类越多，Metaspace 使用的内存越多。与 Metaspace 相关的 JVM 参数有:
XX:MaxMetaspaceSize: 最大的 Metaspace 大小限制【默认无限制】
XX:MetaspaceSize=64M: 初始的 Metaspace 大小。如果 Metaspace 空间不足，将会触发 Full GC。
类空间占用评估，给两个数字可供参考：10K 个类约 90M，15K 个类约 100M。
什么时候回收：分配给一个类的空间，是归属于这个类的类加载器的，只有当这个类加载器卸载的时候，这个空间才会被释放。释放 Metaspace 的空间，并不意味着将这部分空间还给系统内存，这部分空间通常会被 JVM 保留下来。
扩展：参考资料中的<code>Java Metaspace详解</code>，这里完美解释 Metaspace、Compressed Class Space 等。</p></li><li><p>Thread
NMT 中显示的 Thread 部分内存与线程数与 -Xss 参数成正比，一般来说 committed 内存等于 <code>Xss*线程数</code> 。</p></li><li><p>Code
JIT 动态编译产生的 Code 占用的内存。这部分内存主要由-XX:ReservedCodeCacheSize 参数进行控制。</p></li><li><p>Internal
Internal 包含命令行解析器使用的内存、JVMTI、PerfData 以及 Unsafe 分配的内存等等。
需要注意的是，Unsafe_AllocateMemory 分配的内存在 JDK11 之前，在 NMT 中都属于 Internal，但是在 JDK11 之后被 NMT 归属到 Other 中。</p></li><li><p>Symbol
Symbol 为 JVM 中的符号表所使用的内存，HotSpot 中符号表主要有两种：SymbolTable 与 StringTable。
大家都知道 Java 的类在编译之后会生成 Constant pool 常量池，常量池中会有很多的字符串常量，HotSpot 出于节省内存的考虑，往往会将这些字符串常量作为一个 Symbol 对象存入一个 HashTable 的表结构中即 SymbolTable，如果该字符串可以在 SymbolTable 中 lookup（SymbolTable::lookup）到，那么就会重用该字符串，如果找不到才会创建新的 Symbol（SymbolTable::new_symbol）。
当然除了 SymbolTable，还有它的双胞胎兄弟 StringTable（StringTable 结构与 SymbolTable 基本是一致的，都是 HashTable 的结构），即我们常说的字符串常量池。平时做业务开发和 StringTable 打交道会更多一些，HotSpot 也是基于节省内存的考虑为我们提供了 StringTable，我们可以通过 String.intern 的方式将字符串放入 StringTable 中来重用字符串。</p></li><li><p>Native Memory Tracking
Native Memory Tracking 使用的内存就是 JVM 进程开启 NMT 功能后，NMT 功能自身所申请的内存。</p></li></ul><p>观察上面几个区域的分配，没有明显的异常。</p><p>NMT 追踪到的 是 Committed，不一定是 Used，NMT 和 cadvisor 没有找到必然的对应的关系。可以参考 RSS，cadvisor 追踪到 RSS 是 650M，JVM Used 是 500M，还有大约 150M 浮动到哪里去了。</p><p>因为 NMT 只能 Track JVM 自身的内存分配情况，比如：Heap 内存分配，direct byte buffer 等。无法追踪的情况主要包括：</p><ul><li>使用 JNI 调用的一些第三方 native code 申请的内存，比如使用 System.Loadlibrary 加载的一些库。</li><li>标准的 Java Class Library，典型的，如文件流等相关操作（如：Files.list、ZipInputStream 和 DirectoryStream 等）。主要涉及到的调用是 Unsafe.allocateMemory 和 java.util.zip.Inflater.init(Native Method)。</li></ul><p>怎么追踪 NMT 追踪不到的<code>其他内存</code>，目前是安装了 jemalloc 内存分析工具，他能追踪底层内存的分配情况输出报告。</p><p>通过 jemalloc 内存分析工具佐证了上面的结论，Unsafe.allocateMemory 和 java.util.zip.Inflater.init 占了 30%，基本吻合。</p><p><a href=./jemalloc-jvm.png><img src=./jemalloc-jvm.png alt=Jemalloc内存结果></a></p><p>启动 arthas 查看下类调用栈，在 arthas 里执行以下命令：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 先设置unsafe true</span>
</span></span><span class=line><span class=cl>options unsafe <span class=nb>true</span>
</span></span><span class=line><span class=cl><span class=c1># 这个没有</span>
</span></span><span class=line><span class=cl>stack sun.misc.Unsafe allocateMemory
</span></span><span class=line><span class=cl><span class=c1># 这个有</span>
</span></span><span class=line><span class=cl>stack jdk.internal.misc.Unsafe allocateMemory
</span></span><span class=line><span class=cl>stack java.util.zip.Inflater inflate
</span></span></code></pre></div><p>通过上面的命令，能看到 MongoDB 和 netty 一直在申请使用内存。注意：早期的 mongodb client 确实有无法释放内存的 bug，但是在我们场景，长期观察会发现内存申请了逐渐释放了，没有持续增长。回到开头的 ContainerOOM 问题，可能一个原因是流量突增，MongoDB 申请了更多的内存导致 OOM，而不是因为内存不释放。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>ts=2022-12-29 21:20:01;thread_name=ForkJoinPool.commonPool-worker-1;id=22;is_daemon=true;priority=1;TCCL=jdk.internal.loader.ClassLoaders$AppClassLoader@1d44bcfa
</span></span></span><span class=line><span class=cl><span class=go>    @jdk.internal.misc.Unsafe.allocateMemory()
</span></span></span><span class=line><span class=cl><span class=go>        at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:125)
</span></span></span><span class=line><span class=cl><span class=go>        at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:332)
</span></span></span><span class=line><span class=cl><span class=go>        at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:243)
</span></span></span><span class=line><span class=cl><span class=go>        at sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)
</span></span></span><span class=line><span class=cl><span class=go>        at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)
</span></span></span><span class=line><span class=cl><span class=go>        at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)
</span></span></span><span class=line><span class=cl><span class=go>        at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826)
</span></span></span><span class=line><span class=cl><span class=go>        at java.net.Socket$SocketOutputStream.write(Socket.java:1035)
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.internal.connection.SocketStream.write(SocketStream.java:99)
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.internal.connection.InternalStreamConnection.sendMessage(InternalStreamConnection.java:426)
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:444)
</span></span></span><span class=line><span class=cl><span class=go>        ………………………………
</span></span></span><span class=line><span class=cl><span class=go>        at com.mongodb.MongoClientExt$1.execute(MongoClientExt.java:42)
</span></span></span><span class=line><span class=cl><span class=go>        at com.facishare.oms.thirdpush.dao.MongoDao.save(MongoDao.java:31)
</span></span></span><span class=line><span class=cl><span class=go>        ………………………………
</span></span></span></code></pre></div><p>总结 Java 进程内存占用：Total=heap + non-heap + 上面说的这个其他。</p><div class=gdoc-page__anchorwrap><h2 id=jemalloc>jemalloc<a data-clipboard-text=https://l10178.github.io/bits-pieces/java/jvm/#jemalloc class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor jemalloc" href=#jemalloc><svg class="icon link"><use xlink:href="#link"/></svg></a></h2></div><p>jemalloc 是一个比 glibc malloc 更高效的内存池技术，在 Facebook 公司被大量使用，在 FreeBSD 和 FireFox 项目中使用了 jemalloc 作为默认的内存管理器。使用 jemalloc 可以使程序的内存管理性能提升，减少内存碎片。</p><p>比如 Redis 内存分配默认使用的 jemalloc，早期版本安装 redis 是需要手动安装 jemalloc 的，现在 redis 应该是在编译期内置好了。</p><p>原来使用 jemalloc 是为了分析内存占用，通过 jemalloc 输出当前内存分配情况，或者通过 diff 分析前后内存差，大概能看出内存都分给睡了，占了多少，是否有内存无法释放的情况。</p><p>后来参考了这个文章，把 glibc 换成 jemalloc 带来性能提升，降低内存使用，决定一试。</p><p>how we’ve reduced memory usage without changing any code：<a href=https://blog.malt.engineering/java-in-k8s-how-weve-reduced-memory-usage-without-changing-any-code-cbef5d740ad>https://blog.malt.engineering/java-in-k8s-how-weve-reduced-memory-usage-without-changing-any-code-cbef5d740ad</a></p><p>Decreasing RAM Usage by 40% Using jemalloc with Python & Celery: <a href=https://zapier.com/engineering/celery-python-jemalloc/>https://zapier.com/engineering/celery-python-jemalloc/</a></p><p>一个服务，运行一周，观察效果。</p><p>使用 Jemalloc 之前：
<a href=./before-jemalloc.png><img src=./before-jemalloc.png alt=before></a></p><p>使用 Jemalloc 之后（同时调低了 Pod 内存）：
<a href=./after-jemalloc.png><img src=./after-jemalloc.png alt=after></a></p><p>注：以上结果未经生产长期检验。</p><div class=gdoc-page__anchorwrap><h2 id=内存交还给操作系统>内存交还给操作系统<a data-clipboard-text=https://l10178.github.io/bits-pieces/java/jvm/#内存交还给操作系统 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor 内存交还给操作系统" href=#内存交还给操作系统><svg class="icon link"><use xlink:href="#link"/></svg></a></h2></div><p>注意：下面的操作，生产环境不建议这么干。</p><p>默认情况下，OpenJDK 不会主动向操作系统退还未用的内存（不严谨）。看第一张监控的图，会发现运行一段时间后，Pod 的内存使用量一直稳定在 80%&ndash;90%不再波动。</p><p>其实对于 Java 程序，浮动比较大的就是 heap 内存。其他区域 Code、Metaspace 基本稳定</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>#</span> 执行命令获取当前heap情况
</span></span><span class=line><span class=cl><span class=go>jhsdb jmap --heap --pid $(pgrep java)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=gp>#</span>以下为输出
</span></span><span class=line><span class=cl><span class=go>Attaching to process ID 7, please wait...
</span></span></span><span class=line><span class=cl><span class=go>Debugger attached successfully.
</span></span></span><span class=line><span class=cl><span class=go>Server compiler detected.
</span></span></span><span class=line><span class=cl><span class=go>JVM version is 17.0.5+8-LTS
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>using thread-local object allocation.
</span></span></span><span class=line><span class=cl><span class=go>ZGC with 4 thread(s)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>Heap Configuration:
</span></span></span><span class=line><span class=cl><span class=go>   MinHeapFreeRatio         = 40
</span></span></span><span class=line><span class=cl><span class=go>   MaxHeapFreeRatio         = 70
</span></span></span><span class=line><span class=cl><span class=go>   MaxHeapSize              = 1287651328 (1228.0MB)
</span></span></span><span class=line><span class=cl><span class=go>   NewSize                  = 1363144 (1.2999954223632812MB)
</span></span></span><span class=line><span class=cl><span class=go>   MaxNewSize               = 17592186044415 MB
</span></span></span><span class=line><span class=cl><span class=go>   OldSize                  = 5452592 (5.1999969482421875MB)
</span></span></span><span class=line><span class=cl><span class=go>   NewRatio                 = 2
</span></span></span><span class=line><span class=cl><span class=go>   SurvivorRatio            = 8
</span></span></span><span class=line><span class=cl><span class=go>   MetaspaceSize            = 22020096 (21.0MB)
</span></span></span><span class=line><span class=cl><span class=go>   CompressedClassSpaceSize = 1073741824 (1024.0MB)
</span></span></span><span class=line><span class=cl><span class=go>   MaxMetaspaceSize         = 17592186044415 MB
</span></span></span><span class=line><span class=cl><span class=go>   G1HeapRegionSize         = 0 (0.0MB)
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>Heap Usage:
</span></span></span><span class=line><span class=cl><span class=go> ZHeap          used 310M, capacity 710M, max capacity 1228M
</span></span></span></code></pre></div><p>Java 内存不交还，几种情况：</p><ul><li><p>Xms 大于实际需要的内存，比如我们服务设置了 Xms768M，但是实际上只需要 256，高峰期也就 512，到不了 Xms 的值也就无所谓归还。
<a href=./java-heap-use.png><img src=./java-heap-use.png alt=Xms></a></p></li><li><p>上面 jmap 的结果，可以看到 Java 默认的配置 MaxHeapFreeRatio=70，这个 70% Free 几乎很难达到。（另外注意 Xmx==Xms 的情况下这两个参数无效，因为他怎么扩缩都不会突破 Xms 和 Xmx 的限制）</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=go>  MinHeapFreeRatio         = 40
</span></span></span><span class=line><span class=cl><span class=go>  空闲堆空间的最小百分比，计算公式为：HeapFreeRatio =(CurrentFreeHeapSize/CurrentTotalHeapSize) * 100，值的区间为0到100，默认值为 40。如果HeapFreeRatio &lt; MinHeapFreeRatio，则需要进行堆扩容，扩容的时机应该在每次垃圾回收之后。
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=go>  MaxHeapFreeRatio         = 70
</span></span></span><span class=line><span class=cl><span class=go>  空闲堆空间的最大百分比，计算公式为：HeapFreeRatio =(CurrentFreeHeapSize/CurrentTotalHeapSize) * 100，值的区间为0到100，默认值为 70。如果HeapFreeRatio &gt; MaxHeapFreeRatio，则需要进行堆缩容，缩容的时机应该在每次垃圾回收之后。
</span></span></span></code></pre></div></li></ul><p>对于 ZGC，默认是交还给操作系统的。可通过 <code>-XX:+ZUncommit -XX:ZUncommitDelay=300</code> 这两个参数控制（不再使用的内存最多延迟 300s 归还给 OS，线下环境可以改小点）。</p><p>经过调整后的服务，内存提交在 500&ndash;800M 之间浮动，不再是一条直线。</p><p><a href=./memory-dance.png><img src=./memory-dance.png alt=memory-dance></a></p><div class=gdoc-page__anchorwrap><h2 id=问题原因分析和调整>问题原因分析和调整<a data-clipboard-text=https://l10178.github.io/bits-pieces/java/jvm/#问题原因分析和调整 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor 问题原因分析和调整" href=#问题原因分析和调整><svg class="icon link"><use xlink:href="#link"/></svg></a></h2></div><p>回到开头问题，通过上面分析，2G 内存，RSS 其实占用 600M，为什么最终还是 ContainerOOM 了。</p><ol><li>kernel memory 为 0，排除 kernel 泄漏的原因。下面的参考资料里介绍了 kernel 泄露的两种场景。</li><li>Cache 很大，说明文件操作多。搜了一下代码，确实有很多 InputStream 调用没有显式关闭，而且有的 InputSteam Root 引用在 ThreadLocal 里，ThreadLocal 只 init 未 remove。 但是，ThreadLocal 的引用对象是线程池，池不回收，所以这部分可能会无法关闭，但是不会递增，但是 cache 也不能回收。
优化办法：ThreadLocal 中对象是线程安全的，无数据传递，直接干掉 ThreadLocal；显式关闭 InputStream。运行一周发现 cache 大约比优化前低 200&ndash;500M。
ThreadLocal 引起内存泄露是 Java 中很经典的一个场景，一定要特别注意。</li><li>一般场景下，Java 程序都是堆内存占用高，但是这个服务堆内存其实在 200-500M 之间浮动，我们给他分了 768M，从来没有到过这个值，所以调低 Xms。留出更多内存给 JNI 使用。</li><li>线下环境内存分配切换到 jemalloc，待长期观察效果。</li></ol><p>经过上述调整以后：
线下环境 Pod 内存使用量由 1G 降到 600M 作用。线上环境内存使用量在 50%&ndash;80%之间根据流量大小浮动，原来是 85% 居高不小。</p><div class=gdoc-page__anchorwrap><h2 id=参考资料>参考资料<a data-clipboard-text=https://l10178.github.io/bits-pieces/java/jvm/#参考资料 class="gdoc-page__anchor gdoc-page__anchor--right clip" aria-label="Anchor 参考资料" href=#参考资料><svg class="icon link"><use xlink:href="#link"/></svg></a></h2></div><p>Java 进程内存分布：<a href=https://cloud.tencent.com/developer/article/1666640>https://cloud.tencent.com/developer/article/1666640</a></p><p>Java Metaspace 详解：<a href=https://www.javadoop.com/post/metaspace>https://www.javadoop.com/post/metaspace</a></p><p>how we’ve reduced memory usage without changing any code：<a href=https://blog.malt.engineering/java-in-k8s-how-weve-reduced-memory-usage-without-changing-any-code-cbef5d740ad>https://blog.malt.engineering/java-in-k8s-how-weve-reduced-memory-usage-without-changing-any-code-cbef5d740ad</a></p><p>Spring Boot 引起的堆外内存泄漏排查及经验总结：<a href=https://tech.meituan.com/2019/01/03/spring-boot-native-memory-leak.html>https://tech.meituan.com/2019/01/03/spring-boot-native-memory-leak.html</a></p><p>Pod 进程内存缓存分析: <a href=https://zhuanlan.zhihu.com/p/449630026>https://zhuanlan.zhihu.com/p/449630026</a></p><p>Linux 内存中的 Cache 真的能被回收么: <a href=https://cloud.tencent.com/developer/article/1115557>https://cloud.tencent.com/developer/article/1115557</a></p><p>Linux kernel memory 导致的 POD OOM killed: <a href=https://www.cnblogs.com/yannwang/p/13287963.html>https://www.cnblogs.com/yannwang/p/13287963.html</a></p><p>cgroup 内存泄露问题: <a href=https://www.cnblogs.com/leffss/p/15019898.html>https://www.cnblogs.com/leffss/p/15019898.html</a></p></article><div class="gdoc-page__footer flex flex-wrap justify-between"></div></div></main><footer class=gdoc-footer><div class="container flex flex-wrap"><span class=gdoc-footer__item>Copyright © 2020-2022 <a href=https://www.nxest.com class=gdoc-footer__link>nxest.com</a>.</span></div></footer></div><script defer src=/bits-pieces/js/en.search.min.js></script>
<script defer src=/bits-pieces/js/clipboard.min.js></script>
<script>document.addEventListener("DOMContentLoaded",function(){var t=new ClipboardJS(".clip")})</script></body></html>