'use strict';(function(){const indexCfg={};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create(indexCfg);window.geekdocSearchIndex=index;index.add({'id':0,'href':'/bits-pieces/devops/cdr/','title':"使用Visual Studio Code搭建多用户远程IDE",'content':"Securing Visual Studio code-server, support multi-user.\n为 code-server（VS Code Web 版） 增加外部认证，并支持多用户，不同用户的 code-server 实例完全隔离。\n主要为了解决问题：\n  code-server 本身支持配置文件形式的用户名密码认证（截止目前，以后也许会改进）。所以引入了外部认证系统，Google、GitHub、 okta、CAS、Keycloak 等理论上都是支持的。\n  code-server 默认没有数据隔离，所以又加了一层 proxy，为每个用户创建一个（或多个）code-server 实例，以实现用户间的数据隔离。\n  使用开源 Auth Proxy，自己不需要实现复杂的登录流程了，比如 code flow with pkce 对大部分人来说读懂这个协议都很困难。\n  使用组件   keycloak\nRedhat 开源 IAM 系统，提供用户、组织服务，提供标准 OIDC。\n  oauth2-proxy\n认证代理，配合 keycloak 提供完整 OAuth2 Code Flow 认证流程。也可以试试 pomerium，看样子也不错。\n  架构图如下。\n核心逻辑 架构图简单解读，所有过程官方文档都有详细说明，都是配置，以官方配置为准。\n  keycloak 创建 client，使用 OIDC 协议，作为 oauth2-proxy 的 provider。\n  ingress(nginx) 使用 auth_request 指令拦截所有请求，从 oauth2-proxy 进行代理认证，配置可参考 oauth2-proxy auth_request 指导。\nnginx.ingress.kubernetes.io/auth-signin:https://$host/oauth2/start?rd=$escaped_request_uringinx.ingress.kubernetes.io/auth-url:https://$host/oauth2/auth  认证通过后，将用户名/ID 作为标识，通过 Http Header (举例如 X-Forwarded-Preferred-Username) 传入 upstream。\n  gateway(nginx) 从 Header 中获取用户标识，代理到此用户对应的 code-server 实例。\nlocation / { …… proxy_pass http://code-server-$http_x_forwarded_for_preferred_username; }   code-server 各个实例部署时，以免认证方式部署。\n  每个 code-server 实例挂载不同的存储，实现完全隔离。\n  "});index.add({'id':1,'href':'/bits-pieces/devops/mysql5.7to8.0/','title':"MySQL5.7升级至8.0",'content':"MySQL 5.7 升级至 8.0，程序适配以及踩坑记录。\n代码适配方案  mysql-connector-java.jar 升级到 8.0.21 版本。 com.mysql.jdbc.Driver 更换为 com.mysql.cj.jdbc.Driver。 链接 url 里指定 useSSL=false。 链接 url 中显式指定时区，增加 serverTimezone=Asia/Shanghai。  兼容性问题   com.mysql.jdbc.exceptions.jdbc4 在 MySQL Connector 8 中不存在。\n将 mysql-connector-java 版本，升级到 8.0 后，如果项目中有使用 mysql 的 Exception 类，编译时会收到以下错误\nerror: package com.mysql.jdbc.exceptions.jdbc4 does not exist [ERROR] import com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException; [ERROR] import com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException; MySQL 在 8.0 版本重用了现有的 java.sql 异常类，取消了 exceptions.jdbc4 的异常类，整改的异常映射关系如下。\n   5.7 版本异常类 8.0 版本异常类     com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException java.sql.SQLSyntaxErrorException   com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException java.sql.SQLIntegrityConstraintViolationException   com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException java.sql.SQLTransactionRollbackException      数据库关键字扩充问题。\n在适配 MySQL 8.0 的时候遇到如下报错。\nCaused by: java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ', source, i18n_lang' at line 5 问题的原因是：MySQL 新增了一批关键字，在使用过程中，如果字段名称和关键字重复，则需要在字段名称上增加单引号方可使用。RANK 为新增关键字。如下所示：\n\u0026lt;result column=\u0026quot;rank\u0026quot; jdbcType=\u0026quot;INTEGER\u0026quot; property=\u0026quot;rank\u0026quot;/\u0026gt; 新增关键字列表如下。\nACTIVE , ADMIN , ARRAY , ATTRIBUTE , BUCKETS , CLONE , COMPONENT , CUME_DIST (R) , DEFINITION , DENSE_RANK (R) , DESCRIPTION , EMPTY (R) , ENFORCED , ENGINE_ATTRIBUTE , EXCEPT (R) , EXCLUDE , FAILED_LOGIN_ATTEMPTS , FIRST_VALUE (R) , FOLLOWING , GEOMCOLLECTION , GET_MASTER_PUBLIC_KEY , GROUPING (R) , GROUPS (R) , HISTOGRAM , HISTORY , INACTIVE , INVISIBLE , JSON_TABLE (R) , JSON_VALUE , LAG (R) , LAST_VALUE (R) , LATERAL (R) , LEAD (R) , LOCKED , MANAGED , MASTER_COMPRESSION_ALGORITH , MSMASTER_PUBLIC_KEY_PATH , MASTER_TLS_CIPHERSUITES , MASTER_ZSTD_COMPRESSION_LEVEL , MEMBER , NESTED , NETWORK_NAMESPACE , NOWAIT , NTH_VALUE (R) , NTILE (R) , NULLS , OF (R) , OFF , OJ , OLD , OPTIONAL , ORDINALITY , ORGANIZATION , OTHERS , OVER (R) , PASSWORD_LOCK_TIME , PATH , PERCENT_RANK (R) , PERSIST , PERSIST_ONLY , PRECEDING , PRIVILEGE_CHECKS_USER , PROCESS , RANDOM , RANK (R) , RECURSIVE (R) , REFERENCE , REQUIRE_ROW_FORMAT , RESOURCE , RESPECT , RESTART , RETAIN , RETURNING , REUSE , ROLE , ROW_NUMBER (R) , SECONDARY , SECONDARY_ENGINE , SECONDARY_ENGINE_ATTRIBUTE , SECONDARY_LOAD , SECONDARY_UNLOAD , SKIP , SRID , STREAM , SYSTEM (R) , THREAD_PRIORITY , TIES , TLS , UNBOUNDED , VCPU , VISIBLE , WINDOW (R)   "});index.add({'id':2,'href':'/bits-pieces/devops/mysql-large-import/','title':"MySQL大文件导入优化",'content':"项目中需要根据SQL文件导入数据，文件大约20G，正常导入约需要2小时，如何加快导入速度。\n如果一个SQL文件只有一个表的数据，可以直接使用mysql load data infile 语法，速度比较快。\n我们是一个SQL文件包含了很多表，导入过程经过如下设置，20G大约需要40分钟。\n# 进入mysql mysql -u root -p # 创建数据库（如果已经有数据库忽略此步骤） CREATE DATABASE 数据库名; # 设置参数 set sql_log_bin=OFF;//关闭日志 set autocommit=0;//关闭autocommit自动提交模式 0是关闭 1 是开启（默认） set global max_allowed_packet = 20 *1024* 1024 * 1024; # 使用数据库 use 数据库名; # 开启事务 START TRANSACTION; # 导入SQL文件并COMMIT（因为导入比较耗时，导入和COMMIT一行命令） source 文件的路径; COMMIT; "});index.add({'id':3,'href':'/bits-pieces/knives/tldr/','title':"TL;DR",'content':"Too Long; Didn’t Read.\ntldr 根据二八原则，简化了烦琐的 man 指令帮助文档，仅列出常用的该指令的使用方法，让人一看就懂，大多数情况下，给出几个指令的使用 demo 可能正是我们想要的。\n举个例子看下实际运行效果，如下。\n➜ ~ tldr docker Manage Docker containers and images. Some subcommands such as `docker run` have their own usage documentation. More information: \u0026lt;https://docs.docker.com/engine/reference/commandline/cli/\u0026gt;. List currently running docker containers: docker ps List all docker containers (running and stopped): docker ps -a Start a container from an image, with a custom name: docker run --name container_name image Start or stop an existing container: docker start|stop container_name Pull an image from a docker registry: docker pull image Open a shell inside a running container: docker exec -it container_name sh Remove a stopped container: docker rm container_name Fetch and follow the logs of a container: docker logs -f container_name tldr 命令行有多种实现，比如官方推荐的有 npm 和 python。\nnpm install -g tldr pip3 install tldr 个人更喜欢 Rust 版本的实现 tealdeer，支持各系统包管理器和二进制安装，比如 homebrew。\nbrew install tealdeer "});index.add({'id':4,'href':'/bits-pieces/knives/ripgrep/','title':"ripgrep",'content':"ripgrep 简称 rg，是一个面向行的搜索工具，Rust 编写，全平台支持，也是 VS Code 的默认搜索工具。它的搜索性能极高，在大项目中也有着出色的表现，并且默认可以忽略 .gitignore 文件中的内容，非常实用。\n除了作为一个高效的命令行工具使用外，整个项目的设计也不错，另外还是一个学习 Rust 的好项目。\nrg -h 开启探索之旅吧。\n"});index.add({'id':5,'href':'/bits-pieces/knives/','title':"瑞士军刀",'content':"程序员常用优秀工具、软件、类库，上榜都是有理由的。\n"});index.add({'id':6,'href':'/bits-pieces/windows/','title':"Windows",'content':"Windows 下工作常用命令、工具、软件总结。\n"});index.add({'id':7,'href':'/bits-pieces/windows/takeown/','title':"Windows提权 + 设置环境变量",'content':"背景：公司 Windows 办公机受域控安全策略限制，部分文件无权修改，另外开发常用的设置系统环境变量也变灰无法设置。此问题解决方式如下。\n提升文件权限   点击 Windows + X 快捷键 – 选择「命令提示符（管理员）。\n  在 CDM 窗口中执行如下命令。\ntakeown /f C:\\要修复的文件路径   在拿到文件所有权后，还需要使用如下命令获取文件的完全控制权限。\nicacls C:\\要修复的文件路径 /Grant Administrators:F   命令行设置环境变量 Windows 下命令行设置环境变量，方式为 setx 变量名 变量值，变量值带空格等特殊符号的，用引号引起来。\n# 通过命令行设置 Java Home setx JAVA_HOME \u0026#34;C:\\Program Files\\Java\\jdk-11.0.2\u0026#34; # 设置 GO Path setx GOPATH \u0026#34;D:\\workspace\\go\u0026#34; "});index.add({'id':8,'href':'/bits-pieces/kubernetes/cka/','title':"备考 CKA 过程，CKA 真题",'content':"备考 CKA （Certified Kubernetes Administrator）过程，心得，遇见问题，CKA 真题。\n备考环境 备考使用的系统和软件版本如下。\n Ubuntu：20.04 Focal Fossa Kubernetes：1.20.7 kubeadm：1.20.7  安装和使用问题记录 kubeadm 安装问题 安装 kubeadm，国内安装使用阿里镜像源。\n$ cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main 踩坑：因为使用的是 ubuntu 20.04，代号 focal，专门去各个代理镜像源找kubernetes-focal都没有找到，后来发现 google 官方根本没发布对应的版本，只有kubernetes-xenial， k8s 官方文档里 ubuntu 也是用的这一个版本。可以用，就用他吧。\nkubeadm init 时指定使用阿里镜像源（解决国内连不上 k8s.gcr.io 的问题）、指定版本号(安装考试对应的版本，不一定是最新版本)。\n通过指定--image-repository，不需要手动下载镜像重新打 tag，kubeadm 自动使用指定的 repository。\nkubeadm init --image-repository=registry.aliyuncs.com/google_containers \\  --pod-network-cidr=10.244.0.0/16 \\  --kubernetes-version=v1.20.7 解决 scheduler Unhealthy，controller-manager Unhealthy 第一次安装完成后通过 kubectl get cs命令，发现 scheduler Unhealthy，controller-manager Unhealthy。\n$ kubectl get cs NAME STATUS MESSAGE scheduler Unhealthy Get \u0026quot;http://127.0.0.1:10251/healthz\u0026quot;: dial tcp 127.0.0.1:10 controller-manager Unhealthy Get \u0026quot;http://127.0.0.1:10252/healthz\u0026quot;: dial tcp 127.0.0.1:10 查看本机端口，10251 和 10252 都没有启动。\n确认 schedule 和 controller-manager 组件配置是否禁用了非安全端口。\n查看配置文件，路径分别为：/etc/kubernetes/manifests/kube-scheduler.yaml 和 /etc/kubernetes/manifests/kube-controller-manager.yaml 将两个配置文件中 --port=0 注释掉（注释掉是否合适待商量）。\nspec:containers:- command:- kube-scheduler- --authentication-kubeconfig=/etc/kubernetes/scheduler.conf- --authorization-kubeconfig=/etc/kubernetes/scheduler.conf- --bind-address=127.0.0.1# 注释掉port，其他行原样不要动- --port=0解决 master 无法调度 我的环境是单节点，既当 master 又当 worker，kubeadm 安装完成后默认 master 节点是不参与调度的，pod 会一直 pending。\nkubectl describe node 发现 node 的 Taints 里有 node-role.kubernetes.io/master:NoSchedule。\n设置 k8s master 节点参与 POD 调度。\nkubectl taint nodes your-node-name node-role.kubernetes.io/master- 考试心得   刷新浏览器会导致考试被终止。\n  提前演练敲一遍，时间其实挺紧张。\n  官方kubectl Cheat Sheet章节非常有用，必考。\n  命令自动补全 source \u0026lt;(kubectl completion bash)。\n  尽量使用命令创建 Pod、deployment、service。\nkubectl run podname --image=imagename --restart=Never -n namespace kubectl run \u0026lt;deploymentname\u0026gt; --image=\u0026lt;imagename\u0026gt; -n \u0026lt;namespace\u0026gt; kubectl expose \u0026lt;deploymentname\u0026gt; --port=\u0026lt;portNo.\u0026gt; --name=\u0026lt;svcname\u0026gt;   使用 dry-run。\nkubectl run \u0026lt;podname\u0026gt; --image=\u0026lt;imagename\u0026gt; --restart=Never --dry-run -o yaml \u0026gt; title.yaml   使用 kubectl -h 查看各个命令的帮助，很多都在 Examples 里。比如 kubectl expose -h。\n  CKA 真题练习 真题会过时，别指望着刷刷题就通过考试，老老实实学一遍。\n  将所有 pv 按照 name/capacity 排序。\n# sort by name kubectl get pv --sort-by=.metadata.name # sort by capacity kubectl get pv --sort-by=.spec.capacity.storage   deployment 扩容。\nkubectl scale deployment test --replicas=3   Set the node named ek8s-node-1 as unavaliable and reschedule all the pods running on it.\nkubectl cordon ek8s-node-1 # drain node的时候可能出错，根据错误提示加参数 kubectl drain ek8s-node-1 --delete-local-data --ignore-daemonsets --force   Form the pod label name-cpu-loader,find pods running high CPU workloads and write the name of the pod consuming most CPU to the file /opt/KUTR00401/KURT00401.txt(which alredy exists).\n# 注意题目中并没有提namespace，可以先看下这个label的pod在哪个namespace，确定命令中要不要加namespace kubectl top pods -l name=name-cpu-loader --sort-by=cpu echo '排名第一的pod名称' \u0026gt;\u0026gt;/opt/KUTR00401/KUTR00401.txt   "});index.add({'id':9,'href':'/bits-pieces/','title':"bits and pieces",'content':" \nI always have a lot of bits and pieces in my coat pocket.\n只是些日常琐碎笔记，目前看并没什么大用，翻翻看总还是有惊喜。\nLicense Licensed under CC BY-NC 4.0. 2020-2022 nxest.com.\n"});index.add({'id':10,'href':'/bits-pieces/devops/','title':"DevOps",'content':"DevOps 相关的技术选型、算法研究、实战经验教训。\n"});index.add({'id':11,'href':'/bits-pieces/devops/nexus/','title':"Nexus3 批量导入",'content':"应用场景：\n 从一个可联网镜像仓库迁移到另一个无法联网的镜像仓库，涉及 Java 和 JavaScript 依赖包。 源镜像仓库比较大（10T），新镜像仓库只需要部分依赖，不需要全量导入。  Nexus 配置 Repository 在 nexus 上创建 Java 和 NPM Repository。\n注意：\n 创建时类型选择 hosted。 对于 Java Repository 可根据实际情况将 Version Policy 选择 mixed。 Deployment Policy 选择 Allow redeploy，便于后续重复导入。  导入 Java 依赖包   准备 Java 依赖包：将本地 maven 或 gradle 的存储目录(默认$USER/.m2/repository)清空，重新执行命令，获取干净的 repository。\n  从 nexus-repository-import-scripts 获取 mavenimport.sh 文件。\n  将 mavenimport.sh copy 到 repository 文件夹内\n  执行以下命令开始导入，注意换成自己的 admin 密码，修改端口和 Repository。\ncd repository \u0026amp;\u0026amp; chmod +x mavenimport.sh ./mavenimport.sh -u admin -p admin123 -r http://127.0.0.1:8081/repository/your-repo-name/   以上命令开始导入，可能时间较长，可以去界面上刷新下 jdf 的 repository 看看有没有导进去。\n  导入 NPM 依赖包 导入前端 npm 包就是先下载好各个包的 tgz 文件，然后根据 tgz 文件执行 npm publish。\n  准备前端依赖包：使用这个工具 node-tgz-downloader，根据他的说明，先下载好各个 js 的 .tgz文件，默认下载到一个叫 tarballs 的文件夹内，下载成功后大致结果如下，如果 nodejs 版本比较低可能下载失败，可以尝试使用低版本的 node-tgz-downloader。\ntarballs ├── @babel │ ├── code-frame │ │ ├── code-frame-7.12.11.tgz │ │ └── code-frame-7.16.0.tgz │ ├── compat-data │ │ └── compat-data-7.16.4.tgz │ ├── core │ │ └── core-7.16.0.tgz │ ├── generator │ │ └── generator-7.16.0.tgz │ ├── helper-annotate-as-pure │ │ └── helper-annotate-as-pure-7.16.0.tgz   从 nexus-repository-import-scripts 获取 npmimport.sh 文件。\n  将 npmimport.sh copy 到 tarballs 文件夹内。\n  登录 npm 账户，开始执行导入。\nnpm config set registry=http://127.0.0.1:8081/repository/your-npm-repo/ npm adduser --registry=http://127.0.0.1:8081/repository/your-npm-repo/ cd tarballs \u0026amp;\u0026amp; chmod +x npmimport.sh ./npmimport.sh -r http://127.0.0.1:8081/repository/your-npm-repo/   以上命令开始导入，可能时间较长，可以去界面上刷新下 npm 的 repository 看看有没有导进去。\n  客户端使用 Maven 修改 setting.xml，使用私有仓库。\n另外因为部分包不受自己控制，可能缺少 metadata，执行 mvn package 等命令时，注意增加 -c 参数忽略 checksum。\n如：mvn -c clean package -Dmaven.test.skip=true\n"});index.add({'id':12,'href':'/bits-pieces/devops/superset/','title':"Superset 对接 ClickHouse",'content':"Apache Superset，开源数据分析与可视化平台。\n常用功能：\n SQL 查询，作为一个 Web 版本的 ClickHouse、MySQL 等等多种数据源的客户端。 支持对查询结果再进行过滤，直接 copy 或下载查询结果，保存查询条件，分享查询条件等。 丰富的图表设计、分析。内置各种 Charts 图表，支持自定义 Charts 和 Dashboard，需要自己根据业务动手制作，这才是他的主业。 支持用户管理，基于 RBAC 模型的控制权限。（理论上可以对接 keycloak，看 issue 还不完善可能需要改 python 代码）。  配置文件 全部默认配置文件参考superset/config.py。\n如果想自定义，创建一个 superset_config.py 文件放到 python path。\n中文支持 多语言默认是不打开的（因为维护的不好，英文属于正常维护，其他语言属于有空就维护），需要在 superset_config.py 中增加环境变量，支持中文。\n# 默认中文 BABEL_DEFAULT_LOCALE=\u0026#39;zh\u0026#39; # 支持很多其他语言，裁掉一部分，只留了这两个 LANGUAGES = { \u0026#34;en\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;us\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;English\u0026#34;}, \u0026#34;zh\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;cn\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;中文\u0026#34;}, } "});index.add({'id':13,'href':'/bits-pieces/linux/history/','title':"History入门",'content':"Linux 下 history 命令，用好历史命令提高工作效率。\n搜索历史命令 快捷键 Ctrl + r\n非常建议你使用这个命令, 因为当你曾经输过一个很长的命令之后, 当你再次想输入这个命令的时候, 你就可以按下这个快捷键, 然后键入那条长命令的关键词, 然后就会显示出含有那个关键词的命令, 每次按下这个键都会再往上搜一个。可以找个机器实际体会下，确实很常用。\n重复上一次的命令 向上的方向键。上下键翻看历史命令，翻到想执行的命令回车。\n两个叹号: ！！\n还有这个：！-1\n快捷键：Ctrl+p 或 Ctrl+n，向上和向下翻看历史命令，和上下键效果一样。\n从历史记录中执行某个命令 还是沿袭上一个中的 !-n 模式, 其中 n 是一个编号。如下示例，执行了编号为 4 的命令。\n1 service network restart 2 exit 3 id 4 cat /etc/redhat-release # !4 cat /etc/redhat-release 执行曾经的命令中特定开头的 假设你的部分历史命令如下:\n1721 find . -type f 那么, 怎样重复执行 1721 条呢? 除了利用 !-1721 这么麻烦的方法，我们还可以用 !f 这样的姿势。因为开头的 f 是离着最后一条命令最近的, 所以 !f 就执行了它。\n清空历史记录 history -c 在历史记录中显示时间 我们可以用 HISTTIMEFORMAT 这个变量来定义显示历史记录时的时间参数:\n# export HISTTIMEFORMAT='%F %T ' # history 也可以用下面的别名来定义显示历史命令的数量:\nalias h1='history 10' 用 HISTCONTROL 来删除重复的历史记录 下面的例子中, 有三个 pwd 命令, 那么在 history 中就会显示三次 pwd , 有点不那么人性化。\n# pwd # pwd # pwd # history | tail -4 44 pwd 45 pwd 46 pwd 47 history | tail -4 所以我们可以这样修改:\nexport HISTCONTROL=ignoredups 然后就不会出现相邻的重复记录了。\nHistory 扩展和总结 用这个功能可以选择特定的历史记录, 不论是修改还是立即执行, 都可以完成。\n!! 重复上一条命令。\n!10 重复历史记录中第 10 条命令。\n!-2 重复历史记录中倒数第二条命令。\n!string 重复历史记录中最后一条以 string 开头的命令。\n!?string 重复历史记录中最后一条包含 string 的命令。\n"});index.add({'id':14,'href':'/bits-pieces/kubernetes/','title':"Kubernetes",'content':""});index.add({'id':15,'href':'/bits-pieces/linux/','title':"Linux",'content':"Linux 下工作常用命令、工具、软件总结。\nUbuntu Ubuntu 查询指定软件有多少可用版本:\napt-cache madison \u0026lt;\u0026lt;package name\u0026gt;\u0026gt; 安装软件时指定版本号：\n$ apt-get install \u0026lt;\u0026lt;package name\u0026gt;\u0026gt;=\u0026lt;\u0026lt;version\u0026gt;\u0026gt; # apt-get install kubeadm=1.20.7-00 查看 Ubuntu 版本代号：\n$ sb_release -a No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 20.04.2 LTS Release: 20.04 Codename: focal "});index.add({'id':16,'href':'/bits-pieces/linux/shell-coding/','title':"Shell 编程实用句式",'content':"选婿。\n#!/usr/bin/env bash  #!/usr/bin/bash 检查是否以 root 用户执行。\n# check if run as root user if [[ `id -u` -ne 0 ]]; then echo \u0026#34;You need root privileges to run this script.\u0026#34; fi 获取正在执行脚本的绝对路径，注意直接用 $0 或 pwd 获取的可能都不要你想要的。\ncurrent_dir=$(cd `dirname $0`;pwd) 为当前目录包含子目录下所有 .sh 文件增加可执行权限。\nchmod +x `find . -name \u0026#39;*.sh\u0026#39;` 将提示信息显示到终端（控制台），同时也写入到文件里。\nlog_file=/var/log/test.log echo \u0026#34;This line will echo to console and also write to log file.\u0026#34; | tee -a ${log_file} 类似于 Java properties 中 key=value 形式的字符串，取 key 和 value 的值。\nusername_line=\u0026#34;username=test\u0026#34; #key is username key=${username_line%=*} #val is test val=${username_line#*=} 实现 trim 效果。\n#trim string by echo val_trim=$(echo -n ${val}) 字体输出颜色及终端格式控制\n#字体色范围：30-37 echo -e \u0026#34;\\033[30m 黑色字 \\033[0m\u0026#34; echo -e \u0026#34;\\033[31m 红色字 \\033[0m\u0026#34; echo -e \u0026#34;\\033[32m 绿色字 \\033[0m\u0026#34; echo -e \u0026#34;\\033[33m 黄色字 \\033[0m\u0026#34; echo -e \u0026#34;\\033[34m 蓝色字 \\033[0m\u0026#34; echo -e \u0026#34;\\033[35m 紫色字 \\033[0m\u0026#34; echo -e \u0026#34;\\033[36m 天蓝字 \\033[0m\u0026#34; echo -e \u0026#34;\\033[37m 白色字 \\033[0m\u0026#34; #字背景颜色范围：40-47 echo -e \u0026#34;\\033[40;37m 黑底白字 \\033[0m\u0026#34; echo -e \u0026#34;\\033[41;30m 红底黑字 \\033[0m\u0026#34; echo -e \u0026#34;\\033[42;34m 绿底蓝字 \\033[0m\u0026#34; echo -e \u0026#34;\\033[43;34m 黄底蓝字 \\033[0m\u0026#34; echo -e \u0026#34;\\033[44;30m 蓝底黑字 \\033[0m\u0026#34; echo -e \u0026#34;\\033[45;30m 紫底黑字 \\033[0m\u0026#34; echo -e \u0026#34;\\033[46;30m 天蓝底黑字 \\033[0m\u0026#34; echo -e \u0026#34;\\033[47;34m 白底蓝字 \\033[0m\u0026#34; "});index.add({'id':17,'href':'/bits-pieces/linux/tcpdump/','title':"tcpdump",'content':"tcpdump 捕捉网卡eth0流量，并存入到out.pcap文件中。\nsudo tcpdump -vv -s0 -i eth0 -w out.pcap "});index.add({'id':18,'href':'/bits-pieces/kubernetes/tldr/','title':"TL;DR",'content':"复制\u0026ndash;粘贴，这就是生活。\n 复制 secret 到另一个 namespace。\nkubectl get secret mys --namespace=na -oyaml | grep -v \u0026#39;^\\s*namespace:\\s\u0026#39; | kubectl apply --namespace=nb -f - 批量删除 pod。\nkubectl get pods --all-namespaces | grep Evicted | awk \u0026#39;{print $2 \u0026#34; --namespace=\u0026#34; $1}\u0026#39; | xargs kubectl delete pod # Delete by label kubectl delete pod -n idaas-book -l app.kubernetes.io/name=idaas-book 密钥解密。\nkubectl get secret my-creds -n mysql -o jsonpath=\u0026#34;{.data.ADMIN_PASSWORD}\u0026#34; | base64 --decode Docker 保存和导入镜像。\n# save image(s) docker save image:tag image2:tag | gzip \u0026gt;xxx.tar.gz # load images docker load -i xxx.tar.gz "});index.add({'id':19,'href':'/bits-pieces/linux/tar.gz/','title':"文件压缩解压",'content':"常用文件格式 .tar：tar 其实打包（或翻译为归档）文件，本身并没有压缩。在 Linux 里 man tar 可以看到它的描述也是“manipulate tape archives”（tar 最初被用来在磁带上创建档案，现在，用户可以在任何设备上创建档案，只是它的描述还没有改）。\n.gz：gzip 是 GNU 组织开发的一个压缩程序，.gz 结尾的文件就是 gzip 压缩的结果。\n.bz2：bzip2 是一个压缩能力更强的压缩程序，.bz2 结尾的文件就是 bzip2 压缩的结果。\n.Z：compress 也是一个压缩程序。.Z 结尾的文件就是 compress 压缩的结果。\n.zip：使用 zip 软件压缩的文件。\n.tar.gz、.tar.bz2、.tar.xz 等可以理解为打包+压缩的效果，用软件解压可以发现比.gz 多了一层包。gzip 和 bzip2，不能同时压缩多个文件，tar 相当于开个挂加上同时压缩的特效，tar 先归档为一个大文件，而归档为大文件的速度是很快的，测试了一下几乎可以忽略不计。\n除了这些格式外，常见的 deb、exe、msi、rpm、dmg、iso 等安装软件，其实都是经过压缩的，一般情况下没有必要再压缩。而 rar 基本认为是 Windows 平台专属的压缩算法了，各个 Linux 发行版都不自带 rar 压缩解压缩软件，所以可以看到很多软件发行的格式都是 .tar.gz 或 .zip。\n解压缩 根据文件名后缀自行选择解压缩命令。\ntar -xf test.tar gzip -d test.gz gunzip test.gz # -C 直接解压到指定目录 tar -xzf test.tar.gz -C /home bzip2 -d test.bz2 bunzip2 test.bz2 tar -xjf test.tar.bz2 tar -xvJf test.tar.xz 压缩 请根据需要选择压缩算法。\n# 将当前目录下所有jpg格式的文件打包为pictures.tar tar -cf pictures.tar *.jpg # 将Picture目录下所有文件打包并用gzip压缩为pictures.tar.gz tar -czf pictures.tar.gz Picture/ # 将Picture目录下所有文件打包并用bzip2压缩为pictures.tar.bz2 tar -cjf pictures.tar.bz2 Picture/ "});index.add({'id':20,'href':'/bits-pieces/linux/pngquant/','title':"PNG图片批量压缩",'content':"使用 pngquant 命令行批量压缩 PNG 图片。\npngquant 压缩当前目录下全部 PNG 文件，并且默认全覆盖已有。\nfor file in $(ls *.png) do pngquant $file --force --output $file done "});index.add({'id':21,'href':'/bits-pieces/linux/transaction/','title':"数据库事务控制",'content':"数据库事务总结，主要包括数据库事务 ACID 属性介绍、数据库并发问题总结、事务传播行为和隔离级别。\n概念 事务（Transaction）是并发控制的基本单位。它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。标准定义：指作为单个逻辑工作单元执行的一系列操作，而这些逻辑工作单元需要具有原子性， 一致性，隔离性和持久性四个属性，统称为 ACID 特性。\nAtomic（原子性） 事务中包含的操作被看做一个不可分割的逻辑单元，这个逻辑单元中的操作要么全部成功，要么全部失败。\nConsistency（一致性） 事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。只有合法的数据可以被写入数据库，否则事务应该将其回滚到最初状态。关于数据库一致性，更专业的解释请参考专业的书籍。\nIsolation（隔离性） 事务允许多个用户对同一个数据进行并发访问，而不破坏数据的正确性和完整性。同时，并行事务的修改必须与其他并行事务的修改相互独立。\nDurability（持久性） 事务提交后，对系统的影响是永久的。简单理解就是写进去了，不会因为时间、系统环境，关机重启等变化而变化。\n数据库并发问题 数据库是共享资源，通常有许多个事务同时在运行。当多个事务并发地存取数据库时就会产生同时读取和（或）修改同一数据的情况。若对并发操作不加控制就可能会存取和存储不正确的数据，破坏数据库的一致性。所以数据库管理系统必须提供并发控制机制。\n并发操作一般可能带来以下几种问题。为说明问题，先来准备一个例子，假设现在有一个账户表 TBL_BANK_ACCOUNT。\nCREATE TABLE TBL_BANK_ACCOUNT（ AccountId CHAR(4) NOT NULL, -- 银行账号 Username NVARCHAR(63) NOT NULL, -- 用户名 Balance BIGINT NOT NULL -- 余额 ) INSERT INTO TBL_BANK_ACCOUNT VALUES (\u0026#39;9555\u0026#39;, \u0026#39;小明\u0026#39;, 1000) -- 北京分行账号 INSERT INTO dbo.BankAccount VALUES (\u0026#39;9556\u0026#39;, \u0026#39;小明\u0026#39;, 2000) -- 上海分行账号 脏读（Dirty reads） 一个事务读到另一个事务未提交的更新数据。\n   事务 1 取款事务 事务 2 工资转账事务     开始事务     开始事务   查询余额 1000 元    取出 100 变为 900     查询余额为 900   异常发生，事务回滚，余额恢复为 1000     汇入工资 2000 元，余额为 2900    提交事务，最终余额 2900，损失了 100    不可重复读（Non-Repeatable Reads） 在同一个事务内，读取表中的某一行记录，多次读取的结果不同。与幻读区别的重点在于修改，同样的条件，已经读取过的数据，再次读取出来和上一次的值不一样。\n   事务 1 工资计算 事务 2 汇款和通知      开始事务   开始事务 查询工资 2000 元，通知银行汇款 2000   增加加班费 6000 元    提交事务     再次查询工资应发 8000 元，邮件通知员工本月发了 8000 元    提交事务    幻读(Phantom Reads) 一个事务读到另一个事务已提交的新插入的数据,导致前后不一致。与不可重复读有点类似，都是两次读取。区别的重点在于增加或者删除。\n   事务 1 加班录入 事务 2 加班天数统计，计算加班费      开始事务   开始事务 统计员工小明加班 3 天    通知银行发 3 天的加班费   增加一天加班数据    提交事务     再次统计加班天数，是 4 天，通知小明发了 4 天的加班费    提交事务    事务隔离级别 事务隔离级别(Transaction Isolation Level)就是对事务并发控制的等级。标准组织 ANSI 定义了四个隔离级别，读未提交 Read uncommitted、读已提交 Read committed、可重复读 Repeatable read、串行化(序列化)Serializable，这四个级别严格程度越来越高，同时并发性能越来越低。\n读未提交 Read uncommitted 一个事务在执行过程中可以看到其他事务没有提交的记录。\n读已提交 Read committed 只能读已提交的数据。但是读取的数据可以被其他事务修改，这样也就会导致不可重复读。\n可重复读 Repeatable read 所有被 Select 获取的数据都不能被修改。\n序列化 Serializable 所有事务一个接一个的执行。\n数据库并发问题还有常说的第一类更新丢失、第二类更新丢失，为减少概念复杂度，在这里没有列出来。\n各个隔离级别和问题对应，√: 可能出现，×: 不会出现\n    脏读 不可重复读 幻读     Read uncommitted √ √ √   Read committed × √ √   Repeatable read × × √   Serializable × × ×    Oracle 和 SqlServer 默认隔离级别是读已提交，MySql 默认隔离级别是可重复读。\nOracle 只支持 READ COMMITTED 和 SERIALIZABLE 这两种标准隔离级别，另外增加了一个非标准的“只读(read-only)”隔离级别。顺便提一句，他的 Serializable 隔离级别，并不真正阻塞事务的执行（更深层次的理解另外单说）。\n为避免幻读和不可重复读问题，一般是在一个事务里确保只读取数据一次，而不是提高事务的隔离级别。 况且 Oracle 也没法设置 Repeatable read。\n事务传播行为  PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。Spring 默认的事务传播行为。 PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。 PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。 PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。 PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与 PROPAGATION_REQUIRED 类似的操作。  嵌套事务 嵌套是子事务套在父事务中执行，子事务是父事务的一部分，在进入子事务之前，父事务建立一个回滚点，叫 save point，然后执行子事务，这个子事务的执行也算是父事务的一部分，然后子事务执行结束，父事务继续执行。重点就在于那个 save point。看几个问题就明了了：\n 如果子事务回滚，会发生什么？  父事务会回滚到进入子事务前建立的 save point，然后尝试其他的事务或者其他的业务逻辑，父事务之前的操作不会受到影响，更不会自动回滚。\n  如果父事务回滚，会发生什么？  父事务回滚，子事务也会跟着回滚！为什么呢，因为父事务结束之前，子事务是不会提交的，我们说子事务是父事务的一部分，正是这个道理。那么：\n  事务的提交，是什么情况？  是父事务先提交，然后子事务提交，还是子事务先提交，父事务再提交？答案是第二种情况，还是那句话，子事务是父事务的一部分，由父事务统一提交。\n   只读事务 readOnly 概念：从这一点设置的时间点开始到这个事务结束的过程中，其他事务所提交的数据，该事务将看不见。\n应用场合：\n 如果你一次执行单条查询语句，则没有必要启用事务支持，数据库默认支持 SQL 执行期间的读一致性； 如果你一次执行多条查询语句，例如统计查询，报表查询，在这种场景下，多条查询 SQL 必须保证整体的读一致性，否则，在前条 SQL 查询之后，后条 SQL 查询之前，数据被其他用户改变，则该次整体的统计查询将会出现读数据不一致的状态，此时，应该启用事务支持。  总结：\n readonly 并不是所有数据库都支持的，不同的数据库下会有不同的结果。 设置了 readonly 后，connection 都会被赋予 readonly，效果取决于数据库的实现。 在 ORM 中，设置了 readonly 会赋予一些额外的优化，例如在 Hibernate 中，会被禁止 flush 等。 由于只读事务不存在数据的修改，因此数据库将会为只读事务提供一些优化手段，例如 Oracle 对于只读事务，不启动回滚段，不记录回滚 log。  Spring 声明式事务 Spring 提供了编程式事务和声明式事务两种机制。为便于理解，简单回顾下 JDBC 和 Hibernate 的事务管理方式。\n JDBC 方式：  Connection conn = DataSourceUtils.getConnection(); //开启事务 conn.setAutoCommit(false); try { Object retVal = callback.doInConnection(conn); conn.commit(); //提交事务  return retVal; }catch (Exception e) { conn.rollback();//回滚事务  throw e; }finally { conn.close(); }  Hibernate 方式：  Session session = null; Transaction transaction = null; try { session = factory.openSession(); //开启事务  transaction = session.beginTransaction(); transation.begin(); session.save(user); transaction.commit();//提交事务 } catch (Exception e) { transaction.rollback();//回滚事务  return false; }finally{ session.close(); } 看下 Spring 编程式方式：\n//1.获取事务管理器 PlatformTransactionManager txManager =ctx.getBean(\u0026#34;txManager\u0026#34;); //2.定义事务属性 DefaultTransactionDefinition td = new DefaultTransactionDefinition(); td.setIsolationLevel(TransactionDefinition.ISOLATION_READ_COMMITTED); //3开启事务,得到事务状态 TransactionStatus status = txManager.getTransaction(td); try { //4.执行数据库操作  jdbcTempate.queryForInt(\u0026#34;select count(*) from tbl_doc\u0026#34;); //5、提交事务  txManager.commit(status); }catch (Exception e) { //6、回滚事务  txManager.rollback(status); } 可以看到，以上几种方式都比较复杂，需要我们自己处理事务，要做的事情比较多。而 Spring 的声明式事务使用简单，它支持注解和 xml 配置，这里以注解为例。\n@Transactional Spring 声明式事务的使用，一切都落在注解**@Transactional**上。\n先看一个简单的例子，在实现类的加注解，实现事务控制。\n\u0026lt;!-- the service class that we want to make transactional --\u0026gt; @Transactional public class DefaultFooService implements FooService { Foo getFoo(String fooName); Foo getFoo(String fooName, String barName); void insertFoo(Foo foo); void updateFoo(Foo foo); } 使用方法  @Transactional 可用于接口、接口方法、实现类以及类方法上。放在接口或类上，相当于为此接口或类下所有的 public 方法都加了这样一个注解。 Spring 团队的建议是你在具体的类（或类的方法）上使用 @Transactional 注解，而不要使用在类所要实现的任何接口上。你当然可以在接口上使用 @Transactional 注解，但是这将只能当你设置了基于接口的代理时它才生效。因为注解是不能继承的，这就意味着如果你正在使用基于类的代理时，那么事务的设置将不能被基于类的代理所识别，而且对象也将不会被事务代理所包装（将被确认为严重的）。因此，请接受 Spring 团队的建议并且在具体的类上使用@Transactional 注解。 @Transactional 注解应该只被应用到 public 的方法上。 如果你在 protected、private 或者 package-visible 的方法上使用，它也不会报错，也不会生效。 方法的@Transactional 会覆盖类上面声明的事务，也就是方法上的优先级高。  传播行为（Propagation） 所谓事务传播行为就是多个事务方法相互调用时，事务如何在这些方法间传播。Spring 支持 7 种事务传播行为：\n PROPAGATION_REQUIRED 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。 PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY 使用当前的事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则开启一个新的事务。PROPAGATION_NESTED 开始一个 \u0026ldquo;嵌套的\u0026rdquo; 事务, 它是已经存在事务的一个真正的子事务. 潜套事务开始执行时, 它将取得一个 savepoint. 如果这个嵌套事务失败, 我们将回滚到此 savepoint. 潜套事务是外部事务的一部分, 只有外部事务结束后它才会被提交. 嵌套事务回滚不影响外部事务，但外部事务回滚将导致嵌套事务回滚。 使用嵌套事务需要 JDBC3.0 并且事务管理器开启嵌套事务（常用的 JpaTransactionManager 和 HibernateTransactionManager 默认是不开启的），如果没有开启，运行时将抛出异常。  举例，现有用户和地址管理，假如每增加一个新用户就需要自动增加一个与此用户相关的地址（这例子真挫，至今没有见到过这样的需求）。那么代码大致是这个样子：\n//用户管理类 public class UserService { @Resource private UserDao userDao; /** 地址管理类 */ @Resource private AddressService addressService; @Transactional public void save(User user){ //执行sql保存用户信息  userDao.add(user); Address address=new Address();//设置地址信息  //执行sql保存地址信息  addressService.save(address); } } //测试类 public class ApplicationTest { @Resource private UserService userService; @Test public void transactionalTest() { User user = new User(); user.setUsername(\u0026#34;Test-001\u0026#34;); userService.save(user); } } @Transactional 注解加在 UserService.save 和 AddressService.save 两个方法上。\n具体的事务开启和关闭流程，设置 spring 的日志级别为 debug 后，运行，可看到类似于这面这样的日志。这里使用了 Spring data jpa，打印的是 JpaTransactionManager 的日志。UserServices 使用的传播行为是 REQUIRED，AddressService 使用 REQUIRES_NEW。\n- Creating new transaction with name [UserService.save]:PROPAGATION_REQUIRED,ISOLATION_DEFAULT; '' - Opened new EntityManager [org.hibernate.jpa.internal.EntityManagerImpl@16602333] for JPA transaction - Exposing JPA transaction as JDBC transaction [org.springframework.orm.jpa.vendor.HibernateJpaDialect$HibernateConnectionHandle@2dd4a7a9] - Found thread-bound EntityManager [org.hibernate.jpa.internal.EntityManagerImpl@16602333] for JPA transaction - Participating in existing transaction - Found thread-bound EntityManager [org.hibernate.jpa.internal.EntityManagerImpl@16602333] for JPA transaction - Suspending current transaction, creating new transaction with name [AddressService.save] - Opened new EntityManager [org.hibernate.jpa.internal.EntityManagerImpl@28b7646] for JPA transaction - Exposing JPA transaction as JDBC transaction [org.springframework.orm.jpa.vendor.HibernateJpaDialect$HibernateConnectionHandle@40239b34] - Found thread-bound EntityManager [org.hibernate.jpa.internal.EntityManagerImpl@28b7646] for JPA transaction - Participating in existing transaction - Initiating transaction commit - Committing JPA transaction on EntityManager [org.hibernate.jpa.internal.EntityManagerImpl@28b7646] - Closing JPA EntityManager [org.hibernate.jpa.internal.EntityManagerImpl@28b7646] after transaction - Closing JPA EntityManager - Resuming suspended transaction after completion of inner transaction - Initiating transaction commit - Committing JPA transaction on EntityManager [org.hibernate.jpa.internal.EntityManagerImpl@16602333] - Closing JPA EntityManager [org.hibernate.jpa.internal.EntityManagerImpl@16602333] after transaction "});index.add({'id':22,'href':'/bits-pieces/linux/regular-expressions/','title':"Regular Expressions",'content':"正则表达式从入门到放弃 首先来看几个问题：\n 假设有一个字符串“a b c”(中间是一个空格)，想实现按空格分割成一个数组[\u0026ldquo;a\u0026rdquo;,\u0026ldquo;b\u0026rdquo;,\u0026ldquo;c\u0026rdquo;]，用 Java 或 JavaScript 代码如何实现。 原始数组改成\u0026quot;a b c\u0026quot;(中间是 N 个空格)，仍分割成数组[\u0026ldquo;a\u0026rdquo;,\u0026ldquo;b\u0026rdquo;,\u0026ldquo;c\u0026rdquo;]，如何实现。  @Test public void testSplit() { String dotStr = \u0026#34;a.b.c\u0026#34;; assertEquals(dotStr.split(\u0026#34;.\u0026#34;).length, 0); assertEquals(dotStr.split(\u0026#34;\\\\.\u0026#34;).length, 3); String spaceStr=\u0026#34;a b c\u0026#34;;//a与b之间两个空格,b与c之间一个空格  assertEquals(spaceStr.split(\u0026#34; \u0026#34;).length, 4);//一个空格  assertEquals(spaceStr.split(\u0026#34; +\u0026#34;).length, 3);//一个或多个空格  assertEquals(\u0026#34;a b c\u0026#34;.split(\u0026#34; +\u0026#34;).length, 3);//一个或多个空格  assertEquals(\u0026#34;a b c\u0026#34;.split(\u0026#34; +\u0026#34;).length, 3);//一个或多个空格  //顺便提一句,与正则无关,注意下面这个结果,这是Java的split方法自己处理的  assertEquals(\u0026#34;a b c \u0026#34;.split(\u0026#34; \u0026#34;).length, 3);//split会去除最后为空的结果  assertEquals(\u0026#34; a b c\u0026#34;.split(\u0026#34; \u0026#34;).length, 4);//split不会去除前面为空的结果  } 以上就是一个正则的简单应用，其实平时工作中除了编程我们也会经常用到正则，比如搜索所有 Word 文档（*.doc）,在文本编辑器中搜索和替换文字等等。正则表达式经过数年的发展，已经逐渐从模糊而深奥的数学概念，发展成为在计算机各类工具和软件包应用中的主要功能，成为人们工作中的一个利器。\n什么是正则表达式 正则表达式（Regular Expression，regex、regexp，以下简称正则）是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。\n完整的正则表达式由两种字符构成，特殊字符（又译元字符）和普通文字。如果把正则表达式想象成普通的语言，普通文字相当于语言中的单词，而元字符相当于语法。就像一门语言一样，正则是烦琐而强大的，学会之后的应用会让你除了提高效率外，会给你带来绝对的成就感。只要坚持去学，一定会放弃的。\n正则引擎 引擎就相当于语言的编译器，解释器，用来对正则进行语法分析。不同的引擎决定了某个正则能否匹配、在何处匹配，以及匹配成功或报告失败的速度，另外知道正则表达式引擎是如何工作的有助于理解为何某个正则表达式在一个平台好用，换个平台就不好使了。\n正则引擎主要可以分为两大类：一种是 DFA，一种是 NFA。先别管这两个的含义，只需要知道这两个名字。这两种引擎都有了很长的历史，当中也由这两种引擎产生了很多变体。后来又出了一个 POSIX 标准，用来规范这种现象。DFA 是符合这种标准的，NFA 不符合，这样一来，主流的正则引擎又分为 3 类：DFA，传统型 NFA，POSIX NFA。\n关于 DFA 和 NFA 的详细区别，在这里先不提了，因为正则本身就是一个很难懂的语言，一下子提这么多怕大家真的放弃了。但是要了解一点，不同的引擎不同的写法，速度是不一样的，如果对性能要求高的话，还是需要了解引擎的实现方式。Java 和 JavaScript 都是 NFA 类型。除去引擎的不同，不同的语言对正则的实现也有差异，所以下面所有的语法和实例，多以 Java 为主，尽量兼顾 JavaScript，不考虑 Python、C++、Swift 等其他语言。\n基础语法    元字符 说明     ^ 匹配输入字符串开始的位置。   $ 匹配输入字符串结尾的位置。   _ 零次或多次匹配前面的字符或子表达式。例如，zo_ 匹配\u0026quot;z\u0026quot;和\u0026quot;zoo\u0026quot;。* 等效于 {0,}。   + 一次或多次匹配前面的字符或子表达式。例如，\u0026ldquo;zo+\u0026ldquo;与\u0026quot;zo\u0026quot;和\u0026quot;zoo\u0026quot;匹配，但与\u0026quot;z\u0026quot;不匹配。+ 等效于 {1,}。   ? 零次或一次匹配前面的字符或子表达式。例如，\u0026ldquo;do(es)?\u0026ldquo;匹配\u0026quot;do\u0026quot;或\u0026quot;does\u0026quot;中的\u0026quot;do\u0026rdquo;。? 等效于 {0,1}。   {n} n 是非负整数。正好匹配 n 次。例如，\u0026ldquo;o{2}\u0026ldquo;与\u0026quot;Bob\u0026quot;中的\u0026quot;o\u0026quot;不匹配，但与\u0026quot;food\u0026quot;中的两个\u0026quot;o\u0026quot;匹配。   {n,} n 是非负整数。至少匹配 n 次。例如，\u0026ldquo;o{2,}\u0026ldquo;不匹配\u0026quot;Bob\u0026quot;中的\u0026quot;o\u0026rdquo;，而匹配\u0026quot;foooood\u0026quot;中的所有 o。\u0026ldquo;o{1,}\u0026ldquo;等效于\u0026quot;o+\u0026quot;。\u0026ldquo;o{0,}\u0026ldquo;等效于\u0026quot;o*\u0026quot;。   {n,m} M 和 n 是非负整数，其中 n \u0026lt;= m。匹配至少 n 次，至多 m 次。注意：您不能将空格插入逗号和数字之间。   ? 当此字符紧随任何其他限定符（*、+、?、{n}、{n,}、{n,m}）之后时，匹配模式是\u0026quot;非贪心的\u0026rdquo;。\u0026ldquo;非贪心的\u0026quot;模式匹配搜索到的、尽可能短的字符串，而默认的\u0026quot;贪心的\u0026quot;模式匹配搜索到的、尽可能长的字符串。例如，在字符串\u0026quot;oooo\u0026quot;中，\u0026ldquo;o+?\u0026ldquo;只匹配单个\u0026quot;o\u0026rdquo;，而\u0026quot;o+\u0026ldquo;匹配所有\u0026quot;o\u0026rdquo;。   . 匹配除\u0026rdquo;\\r\\n\u0026quot;之外的任何单个字符。若要匹配包括\u0026rdquo;\\r\\n\u0026quot;在内的任意字符，请使用诸如\u0026rdquo;[\\s\\S]\u0026ldquo;之类的模式。   (pattern) 匹配 pattern 并捕获该匹配的子表达式。可以使用 $0…$9 属性从结果\u0026quot;匹配\u0026quot;集合中检索捕获的匹配。若要匹配括号字符 ( )，请使用\u0026rdquo;(\u0026ldquo;或者\u0026rdquo;)\u0026quot;。   x│y 匹配 x 或 y。例如'(z│f)ood' 匹配\u0026quot;zood\u0026quot;或\u0026quot;food\u0026rdquo;。   [xyz] 字符集。匹配包含的任一字符。例如，\u0026quot;[abc]\u0026ldquo;匹配\u0026quot;plain\u0026quot;中的\u0026quot;a\u0026rdquo;。   [^xyz] 反向字符集。匹配未包含的任何字符。例如，\u0026quot;[^abc]\u0026ldquo;匹配\u0026quot;plain\u0026quot;中\u0026quot;p\u0026rdquo;，\u0026ldquo;l\u0026rdquo;，\u0026ldquo;i\u0026rdquo;，\u0026ldquo;n\u0026rdquo;。   [a-z] 字符范围。匹配指定范围内的任何字符。例如，\u0026quot;[a-z]\u0026ldquo;匹配\u0026quot;a\u0026quot;到\u0026quot;z\u0026quot;范围内的任何小写字母。   [^a-z] 反向范围字符。匹配不在指定的范围内的任何字符。例如，\u0026quot;[^a-z]\u0026ldquo;匹配任何不在\u0026quot;a\u0026quot;到\u0026quot;z\u0026quot;范围内的任何字符。   \\b 匹配一个字边界，即字与空格间的位置。例如，\u0026ldquo;er\\b\u0026quot;匹配\u0026quot;never\u0026quot;中的\u0026quot;er\u0026rdquo;，但不匹配\u0026quot;verb\u0026quot;中的\u0026quot;er\u0026rdquo;。   \\B 非字边界匹配。\u0026ldquo;er\\B\u0026quot;匹配\u0026quot;verb\u0026quot;中的\u0026quot;er\u0026rdquo;，但不匹配\u0026quot;never\u0026quot;中的\u0026quot;er\u0026rdquo;。   \\d 数字字符匹配。等效于 [0-9]。   \\D 非数字字符匹配。等效于 [^0-9]。   \\f 换页符匹配。等效于 \\x0c 和 \\cL。   \\n 换行符匹配。等效于 \\x0a 和 \\cJ。   \\r 匹配一个回车符。等效于 \\x0d 和 \\cM。   \\s 匹配任何空白字符，包括空格、制表符、换页符等。   \\S 匹配任何非空白字符。   \\t 制表符匹配。   \\w 匹配任何字类字符，包括下划线。与\u0026rdquo;[A-Za-z0-9_]\u0026ldquo;等效。   \\W 与任何非单词字符匹配。与\u0026rdquo;[^a-za-z0-9_]\u0026ldquo;等效。    运算符优先级 正则表达式从左到右进行计算，并遵循优先级顺序，这与算术表达式非常类似。相同优先级的从左到右进行运算，不同优先级的运算先高后低。下表从最高到最低说明了各种正则表达式运算符的优先级顺序.\n"});index.add({'id':23,'href':'/bits-pieces/categories/','title':"Categories",'content':""});index.add({'id':24,'href':'/bits-pieces/tags/','title':"Tags",'content':""});})();